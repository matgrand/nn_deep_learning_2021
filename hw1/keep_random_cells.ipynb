{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### FINAL TRAINING\n",
    "# ### TRAINING LOOP\n",
    "# # Define the loss function\n",
    "# loss_fn = nn.MSELoss()\n",
    "# # Define the optimizer\n",
    "# optimizer = optim.Adam(net.parameters(), lr=1e-3) #1e-3\n",
    "\n",
    "\n",
    "\n",
    "# num_epochs = 2000 #25000\n",
    "# train_loss_log = []\n",
    "# val_loss_log = []\n",
    "# for epoch_num in tqdm(range(num_epochs)):\n",
    "\n",
    "#     ### TRAIN\n",
    "#     train_loss= []\n",
    "#     net.train() # Training mode (e.g. enable dropout, batchnorm updates,...)\n",
    "#     for sample_batched in train_dataloader:\n",
    "#         # Move data to device\n",
    "#         x_batch = sample_batched[0].to(device)\n",
    "#         label_batch = sample_batched[1].to(device)\n",
    "\n",
    "#         # Forward pass\n",
    "#         out = net(x_batch)\n",
    "\n",
    "#         # Compute loss\n",
    "#         loss = loss_fn(out, label_batch)\n",
    "\n",
    "#         # Backpropagation\n",
    "#         net.zero_grad() # Zero the gradients\n",
    "#         loss.backward() # Compute the gradients\n",
    "\n",
    "#         # Update the weights\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Save train loss for this batch\n",
    "#         loss_batch = loss.detach().cpu().numpy()  # .detach() to avoid keeping the gradient in memory\n",
    "#         train_loss.append(loss_batch)\n",
    "\n",
    "#     # Save average train loss\n",
    "#     train_loss = np.mean(train_loss)\n",
    "#     print(f'Train loss: {train_loss}')\n",
    "#     train_loss_log.append(train_loss)\n",
    "\n",
    "#     ### VALIDATION\n",
    "#     val_loss= []\n",
    "#     net.eval() # Evaluation mode (e.g. disable dropout, batchnorm,...)\n",
    "#     with torch.no_grad(): # Disable gradient tracking\n",
    "#         for sample_batched in val_dataloader:\n",
    "#             # Move data to device\n",
    "#             x_batch = sample_batched[0].to(device)\n",
    "#             label_batch = sample_batched[1].to(device)\n",
    "\n",
    "#             # Forward pass\n",
    "#             out = net(x_batch)\n",
    "\n",
    "#             # Compute loss\n",
    "#             loss = loss_fn(out, label_batch)\n",
    "\n",
    "#             # Save val loss for this batch\n",
    "#             loss_batch = loss.detach().cpu().numpy()\n",
    "#             val_loss.append(loss_batch)\n",
    "\n",
    "#         # Save average validation loss\n",
    "#         val_loss = np.mean(val_loss)\n",
    "#         print(f\"AVERAGE VAL LOSS: {np.mean(val_loss)}\")\n",
    "#         val_loss_log.append(val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "very good regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TRAINING very good\n",
    "torch.manual_seed(42)\n",
    "net = Net(1, 16, 16, 1, droput_rate=0.0)\n",
    "# Define the loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-3) #1e-3\n",
    "num_epochs = 20000 #25000\n",
    "# Train\n",
    "train_loss_log, val_loss_log, net = train_model(net, train_dataloader, val_dataloader, optimizer, loss_fn, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### @add DROPOUT LAYERS\n",
    "#basic 2-layer network with 2 hidden layers, sigmoid activation function\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, Ni, Nh1, Nh2, No, droput_rate=0):\n",
    "        \"\"\"\n",
    "        Ni - Input size\n",
    "        Nh1 - Neurons in the 1st hidden layer\n",
    "        Nh2 - Neurons in the 2nd hidden layer\n",
    "        No - Output size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        print('Network initialized')\n",
    "        self.fc1 = nn.Linear(in_features=Ni, out_features=Nh1)\n",
    "        self.dp1 = nn.Dropout(p=droput_rate)\n",
    "        self.fc2 = nn.Linear(in_features=Nh1, out_features=Nh2)\n",
    "        self.dp2 = nn.Dropout(p=droput_rate)\n",
    "        self.out = nn.Linear(in_features=Nh2, out_features=No)\n",
    "        self.act = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, additional_out=False):\n",
    "        x = self.act(self.fc1(x))\n",
    "        # x = self.dp1(x)\n",
    "        x = self.act(self.fc2(x))\n",
    "        # x = self.dp2(x)\n",
    "        x = self.out(x)\n",
    "        # x = self.act(self.fc1(x)) ##original\n",
    "        # x = self.act(self.fc2(x)) ##original\n",
    "        # x = self.out(x)           ##original\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network initialized\n",
      "Training started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [00:11<00:00, 126.78it/s]\n"
     ]
    }
   ],
   "source": [
    "# ## TRAINING\n",
    "# torch.manual_seed(42)\n",
    "# k_folds = 2\n",
    "# net = Net(1, 16, 16, 1, activation_fn=nn.Tanh(),droput_rate=0)\n",
    "# act_fns = [nn.Sigmoid(), nn.Tanh(), nn.ReLU(), nn.Softsign(), nn.Softplus()]\n",
    "# hidden_layers = [16, 32, 64]\n",
    "# # Define the loss function\n",
    "# loss_fn = nn.MSELoss()\n",
    "# loss_fns = [nn.L1Loss(), nn.HuberLoss(), nn.MSELoss()]\n",
    "# # Define the optimizer\n",
    "# optimizer = optim.Adam(net.parameters(), lr=5e-3)\n",
    "# optimizers = [optim.SGD(net.parameters(), lr=5e-3), optim.Adam(net.parameters(), lr=5e-3), optim.RMSprop(net.parameters(), lr=5e-3)]\n",
    "# lr_rates = [15e-3, 10e-3, 8e-3, 5e-3, 2e-3, 1e-3, 5e-4]\n",
    "# num_epochs = 1500 \n",
    "# # Train\n",
    "# train_loss_log, val_loss_log, net = train_model(net, train_dataloader, val_dataloader, optimizer, loss_fn, num_epochs, enable_early_stopping=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cee89b7c6bc96453738565335b56b694d8a30ac65e979633b683f8408c8233c6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('dl_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
