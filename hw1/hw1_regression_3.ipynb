{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOMEWORK 1 NEURAL NETWORKS AND DEEP LEARNING\n",
    "\n",
    "---\n",
    "A.A. 2021/22 (6 CFU) - Dr. Alberto Testolin, Dr. Umberto Michieli\n",
    "---\n",
    "Student: Matteo Grandin\n",
    "---\n",
    "id: 2020374"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold # this module is useful to split data into training and test sets\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import pickle\n",
    "\n",
    "# Check if the GPU is available\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Training device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell I just get the datasets for the regression task, load them, get a sample, and plot the 2 datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dataset loading\n",
    "!wget -P regression_dataset https://gitlab.dei.unipd.it/michieli/nnld-2021-22-lab-resources/-/raw/main/homework1/train_data.csv\n",
    "!wget -P regression_dataset https://gitlab.dei.unipd.it/michieli/nnld-2021-22-lab-resources/-/raw/main/homework1/test_data.csv \n",
    "\n",
    "#load the datasets\n",
    "train_df = pd.read_csv('regression_dataset/train_data.csv')\n",
    "test_df = pd.read_csv('regression_dataset/test_data.csv')\n",
    "\n",
    "#plot train (+validation) set\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "plt.scatter(train_df.input, train_df.label, label='Training points')\n",
    "plt.xlabel('input')\n",
    "plt.ylabel('label')\n",
    "#plot test set on the same plot\n",
    "plt.scatter(test_df.input, test_df.label, label='Test points')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f'Train set size: {train_df.shape[0]}')\n",
    "print(f'Test set size: {test_df.shape[0]}')\n",
    "\n",
    "#divide the training set into training and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_val, y_train, y_val = train_test_split(train_df.input, train_df.label, test_size=0.2, random_state=42)\n",
    "\n",
    "### Train data\n",
    "with open('train_data.csv', 'w') as f:\n",
    "    #write the header\n",
    "    f.write('input,label\\n')\n",
    "    data = [f\"{x},{y}\" for x, y in zip(x_train, y_train)]\n",
    "    f.write('\\n'.join(data))\n",
    "    #write footer\n",
    "    f.write('\\n')\n",
    "    \n",
    "### Validation data\n",
    "with open('val_data.csv', 'w') as f:\n",
    "    #write the header\n",
    "    f.write('input,label\\n')\n",
    "    data = [f\"{x},{y}\" for x, y in zip(x_val, y_val)]\n",
    "    f.write('\\n'.join(data))\n",
    "    #write footer\n",
    "    f.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset \n",
    "class CsvDataset(Dataset):\n",
    "\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.transform = transform\n",
    "        # Read the file and split the lines in a list\n",
    "        with open(csv_file, 'r') as f:\n",
    "            lines = f.read().split('\\n')\n",
    "        # Get x and y values from each line and append to self.data\n",
    "        self.data = []\n",
    "        # remove header and footer\n",
    "        lines = lines[1:-1]\n",
    "        for line in lines:\n",
    "            sample = line.split(',')\n",
    "            self.data.append((float(sample[0]), float(sample[1])))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Our sample is the element idx of the list self.data\n",
    "        sample = self.data[idx]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "\n",
    "# simple pytorch tensor conversion\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert sample to Tensors.\"\"\"\n",
    "    def __call__(self, sample):\n",
    "        x, y = sample\n",
    "        return (torch.tensor([x]).float(),\n",
    "                torch.tensor([y]).float())\n",
    "\n",
    "composed_transform = transforms.Compose([ToTensor()])\n",
    "\n",
    "# create the dataset\n",
    "train_dataset = CsvDataset('train_data.csv', transform=composed_transform)\n",
    "val_dataset = CsvDataset('val_data.csv', transform=composed_transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=0) #batch size = 8\n",
    "val_dataloader  = DataLoader(val_dataset,  batch_size=len(val_dataset), shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot it's very noticeable that the training points are not sampled in the same way of the test points, for example, around -2 and +2 there are 2 \"blank\" spots, with no samples whatsoever, moreover the samples in the training set seem to have an higher variance. The model will need to be able to generalize. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a fully connected feed-forward network, different architectures will be considered, different activation functions will be considered in the hidden layers, no activation function will be considered for the output layer since we are dealing with a regression problem and we don't want to limit the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### @add DROPOUT LAYERS\n",
    "#basic 2-layer network with 2 hidden layers, sigmoid activation function\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, Ni, Nh1, Nh2, No, activation_fn=nn.Sigmoid(), droput_rate=0):\n",
    "        \"\"\"\n",
    "        Ni - Input size\n",
    "        Nh1 - Neurons in the 1st hidden layer\n",
    "        Nh2 - Neurons in the 2nd hidden layer\n",
    "        No - Output size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features=Ni, out_features=Nh1)\n",
    "        self.dp1 = nn.Dropout(p=droput_rate)\n",
    "        self.fc2 = nn.Linear(in_features=Nh1, out_features=Nh2)\n",
    "        self.dp2 = nn.Dropout(p=droput_rate)\n",
    "        self.out = nn.Linear(in_features=Nh2, out_features=No)\n",
    "        self.act = activation_fn\n",
    "        \n",
    "    def forward(self, x, additional_out=False):\n",
    "        x = self.act(self.fc1(x))\n",
    "        x = self.act(self.fc2(x))\n",
    "        x = self.out(x)\n",
    "        # x = self.act(self.fc1(x)) ##original\n",
    "        # x = self.act(self.fc2(x)) ##original\n",
    "        # x = self.out(x)           ##original\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#useful functions\n",
    "def plot_save_losses_and_output(val_losses, net, title):\n",
    "    #first subplot\n",
    "    fig = plt.figure(figsize=(15,3))\n",
    "    #set title \n",
    "    plt.suptitle(title)\n",
    "    p1 = plt.subplot(1, 2, 1)\n",
    "    p1.plot(val_losses, label=\"Validation loss\")\n",
    "    p1.set_xlabel(\"Epochs\")\n",
    "    p1.set_ylabel(\"Loss\")\n",
    "    p1.set_ylim(0, 2)\n",
    "    p1.legend()\n",
    "\n",
    "    # Input vector\n",
    "    x_vec = torch.linspace(-5,5,1000)\n",
    "    x_vec = x_vec.to(device)\n",
    "    x_vec = x_vec.unsqueeze(-1)  # Adding a dimension to the input vector\n",
    "\n",
    "    net.eval()\n",
    "    with torch.no_grad(): # turn off gradients computation\n",
    "        y_vec = net(x_vec)\n",
    "\n",
    "    # Convert x_vec and y_vec to numpy one dimensional arrays\n",
    "    x_vec = x_vec.squeeze().cpu().numpy()\n",
    "    y_vec = y_vec.squeeze().cpu().numpy()\n",
    "\n",
    "    # Plot output\n",
    "    p2 = plt.subplot(1, 2, 2)\n",
    "    #show the points from the test dataset\n",
    "    p2.scatter(test_df.input, test_df.label, label='Test points')\n",
    "    # learnt function\n",
    "    p2.plot(x_vec, y_vec, label='Network output', color='red')\n",
    "\n",
    "    #plt.plot(x_vec, true_model, label='True model')\n",
    "    p2.set_xlabel('x')\n",
    "    p2.set_ylabel('y')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    #save fig\n",
    "    fig.savefig(f'images/{title}.eps', format='eps', dpi=1000, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def reset_weights(model):\n",
    "    for layer in model.modules():\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            layer.reset_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TRAINING FUNCTION\n",
    "def train_model(net, train_dataloader, val_dataloader, optimizer, loss_fn, num_epochs=1, enable_early_stopping=True):\n",
    "    net = net.to(device)\n",
    "    train_loss_log = []\n",
    "    val_loss_log = []\n",
    "    min_validation_loss = float('inf')\n",
    "    early_stopping_counter = 0\n",
    "    for epoch_num in tqdm(range(num_epochs)):\n",
    "\n",
    "        ### TRAIN\n",
    "        train_loss= []\n",
    "        net.train() # Training mode (e.g. enable dropout, batchnorm updates,...)\n",
    "        for sample_batched in train_dataloader:\n",
    "            # Move data to device\n",
    "            x_batch = sample_batched[0].to(device)\n",
    "            label_batch = sample_batched[1].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            out = net(x_batch)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_fn(out, label_batch)\n",
    "\n",
    "            # Backpropagation\n",
    "            net.zero_grad() # Zero the gradients\n",
    "            loss.backward() # Compute the gradients\n",
    "\n",
    "            # Update the weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # Save train loss for this batch\n",
    "            loss_batch = loss.detach().cpu().numpy()  # .detach() to avoid keeping the gradient in memory\n",
    "            train_loss.append(loss_batch)\n",
    "\n",
    "        # Save average train loss\n",
    "        train_loss = np.mean(train_loss)\n",
    "        # print(f'Train loss: {train_loss}')\n",
    "        train_loss_log.append(train_loss)\n",
    "\n",
    "        ### VALIDATION\n",
    "        if val_dataloader is not None:\n",
    "            net.eval()\n",
    "            val_loss= []\n",
    "            net.eval() # Evaluation mode (e.g. disable dropout, batchnorm,...)\n",
    "            with torch.no_grad(): # Disable gradient tracking\n",
    "                for sample_batched in val_dataloader:\n",
    "                    # Move data to device\n",
    "                    x_batch = sample_batched[0].to(device)\n",
    "                    label_batch = sample_batched[1].to(device)\n",
    "\n",
    "                    # Forward pass\n",
    "                    out = net(x_batch)\n",
    "\n",
    "                    # Compute loss\n",
    "                    loss = loss_fn(out, label_batch)\n",
    "\n",
    "                    # Save val loss for this batch\n",
    "                    loss_batch = loss.detach().cpu().numpy()\n",
    "                    val_loss.append(loss_batch)\n",
    "\n",
    "                # Save average validation loss\n",
    "                val_loss = np.mean(val_loss)\n",
    "                # print(f\"AVERAGE VAL LOSS: {np.mean(val_loss)}\")\n",
    "                val_loss_log.append(val_loss)\n",
    "\n",
    "                ## early stopping\n",
    "                if enable_early_stopping:\n",
    "                    if val_loss < min_validation_loss:\n",
    "                        min_validation_loss = val_loss\n",
    "                        torch.save(net.state_dict(), 'training/best_regression_val_model.pt')\n",
    "                    if epoch_num > 10 and np.mean(val_loss_log[-3:-1]) > 1.2*min_validation_loss:\n",
    "                        early_stopping_counter += 1\n",
    "                        if early_stopping_counter > 5:\n",
    "                            early_stopping_counter += 1\n",
    "                            print(\"Early stopping\")\n",
    "                            #load previous best model\n",
    "                            net.load_state_dict(torch.load('training/best_regression_val_model.pt'))\n",
    "                            break\n",
    "                    else:\n",
    "                        early_stopping_counter = 0\n",
    "                \n",
    "    return train_loss_log, val_loss_log, net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cross validation training\n",
    "torch.manual_seed(42)\n",
    "## paramters\n",
    "#batch size\n",
    "batch_sizes = [8]\n",
    "#activation function\n",
    "activation_fns = [nn.Tanh()] # [nn.Sigmoid(), nn.Tanh(), nn.ReLU(), nn.Softplus()]\n",
    "activation_fns_strings = ['Tanh'] #['Sigmoid', 'Tanh', 'ReLU', 'Softplus']\n",
    "#loss function\n",
    "loss_fns = [nn.L1Loss(), nn.MSELoss()] #[nn.L1Loss(), nn.HuberLoss(), nn.MSELoss()]\n",
    "loss_fns_strings = ['L1', 'MSE'] #['L1', 'Huber', 'MSE']\n",
    "#learning rate\n",
    "learning_rates = [15e-3, 3e-3, 1e-3] ##= [15e-3, 10e-3, 8e-3, 3e-3, 1e-3, 5e-4]\n",
    "# number of hidden layers\n",
    "hidden_layers = [16]\n",
    "#optimizer\n",
    "optimizers = [] # will be filled later inside the loop\n",
    "optimizers_strings = ['SGDMomentum', 'Adam'] # ['SGD', 'SGDMomentum', 'Adam']\n",
    "#number of epochs\n",
    "num_epochs = 1000\n",
    "#k-folds\n",
    "k_folds = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train loop\n",
    "losses = np.zeros((len(batch_sizes), len(activation_fns), len(loss_fns), len(learning_rates), len(hidden_layers), len(optimizers_strings), num_epochs))\n",
    "num_tot_combinations = len(batch_sizes)*len(activation_fns)*len(loss_fns)*len(learning_rates)*len(hidden_layers)*len(optimizers_strings)\n",
    "comb = 0\n",
    "for a, batch_size in enumerate(batch_sizes):\n",
    "    for b, (act_fn, act_fn_str) in enumerate(zip(activation_fns, activation_fns_strings)):\n",
    "        for c, (loss_fn, loss_fn_str) in enumerate(zip(loss_fns, loss_fns_strings)):\n",
    "            for d, lr in enumerate(learning_rates):\n",
    "                for e, hidden_layer in enumerate(hidden_layers):\n",
    "                    for f, optimizer_str in enumerate(optimizers_strings):\n",
    "                        print(f\"_____________________{comb+1}/{num_tot_combinations}____________________\")\n",
    "                        print(f\"bs: {batch_size}, act_fn: {act_fn_str}, loss_fn: {loss_fn_str}, lr: {lr}, hlayers: {hidden_layer}, opt: {optimizer_str}\")\n",
    "                        comb+=1\n",
    "                        # Create the model\n",
    "                        net = Net(1, hidden_layer, hidden_layer, 1, activation_fn=act_fn,droput_rate=0)\n",
    "                        #initialize the optimizer\n",
    "                        optimizer = None\n",
    "                        if optimizer_str == 'SGD':\n",
    "                            optimizer = optim.SGD(net.parameters(), lr=lr)\n",
    "                        elif optimizer_str == 'SGDMomentum':\n",
    "                            optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9)\n",
    "                        elif optimizer_str == 'Adam':\n",
    "                            optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "                        else:\n",
    "                            print(\"Error, wrong optimizer\")\n",
    "                        # Define the loss function\n",
    "                        loss_fn = loss_fn\n",
    "                        # Train\n",
    "                        kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "                        combination_val_losses = np.zeros(num_epochs)\n",
    "                        for fold, (train_ids, val_ids) in enumerate(kfold.split(train_dataset)):\n",
    "                            # reset network parameters\n",
    "                            reset_weights(net)\n",
    "                            train_dataloader = DataLoader(train_dataset, batch_size=batch_size, \n",
    "                                            sampler=torch.utils.data.SubsetRandomSampler(train_ids))\n",
    "                            val_dataloader = DataLoader(train_dataset, batch_size=256, \n",
    "                                            sampler=torch.utils.data.SubsetRandomSampler(val_ids))\n",
    "                            train_loss_log, val_loss_log, net = train_model(net, train_dataloader, val_dataloader, \n",
    "                                            optimizer, loss_fn, num_epochs, enable_early_stopping=False)\n",
    "                            combination_val_losses += np.array(val_loss_log)/k_folds\n",
    "                            \n",
    "                        #store the losses\n",
    "                        losses[a,b,c,d,e,f,:] = combination_val_losses\n",
    "\n",
    "                        #complete training on the full dataset\n",
    "                        reset_weights(net)\n",
    "                        dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "                        _, _, net = train_model(net, dataloader, None, optimizer, loss_fn, num_epochs, enable_early_stopping=False)\n",
    "                        #print the loss\n",
    "                        model_name = 'regression_model_' + str(batch_size) + '_' + act_fn_str + '_' + loss_fn_str + '_' \\\n",
    "                                + optimizer_str + '_' + str(batch_size) + '_' + str(hidden_layer) + '_' + str(lr)\n",
    "                        plot_save_losses_and_output(combination_val_losses, net, model_name)\n",
    "                        # Save model\n",
    "                        torch.save(net.state_dict(), 'training/' + model_name + '.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "- using a big batch size (256), and training for a lot of epochs the model overfits, I therefore implemented early stopping in order to stop the training when the validation loss exceeds 20% of the minimum validation loss so far\n",
    "- using a big batch size is not effective (with adam at least), it's better to use small batch sizes to achieve faster convergence\n",
    "- I achieved very good results with 11k iterations (early stopped) 256 bs. lr 1e-3 adam mse. \n",
    "- Droput is very bad in this situation (why?)\n",
    "- L1Loss works very well, much better than MSE, but needs more iterations\n",
    "- HuberLoss also works well, but also needs more iterations\n",
    "\n",
    "Optimizers:\n",
    "- Adam is good lr around 5e-3, lower if we do more iterations\n",
    "- SGD with momentum needs higher lr, \n",
    "- SGD without momentum bad\n",
    "- RMSprop similar to Adam but slightly worse\n",
    "\n",
    "Activation functions\n",
    "- Relu is not smooth and it's not correct for this smooth function\n",
    "- Tanh and Sigmoid give similar results, Tanh slightly better\n",
    "- Softsign similar to Tanh but worse\n",
    "- nn.Softplus() kinda good\n",
    "- Softshrink a kinda smoother relu but still worse\n",
    "\n",
    "Number of hidden layers\n",
    "- 16/32 is a good number\n",
    "- the bigger the number the faster the convergence, but the higher the risk of overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot losses\n",
    "plt.figure(figsize=(14,8))\n",
    "plt.semilogy(train_loss_log, label='Train loss')\n",
    "plt.semilogy(val_loss_log, label='Validation loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## @add You need to visualize Weight histograms, activation profiles, and the final output of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input vector\n",
    "x_vec = torch.linspace(-5,5,1000)\n",
    "x_vec = x_vec.to(device)\n",
    "x_vec = x_vec.unsqueeze(-1)  # Adding a dimension to the input vector\n",
    "print(f\"Input shape: {x_vec.shape}\")\n",
    "\n",
    "net.eval()\n",
    "with torch.no_grad(): # turn off gradients computation\n",
    "    y_vec = net(x_vec)\n",
    "print(f\"Output shape: {y_vec.shape}\")\n",
    "\n",
    "# Convert x_vec and y_vec to numpy one dimensional arrays\n",
    "x_vec = x_vec.squeeze().cpu().numpy()\n",
    "y_vec = y_vec.squeeze().cpu().numpy()\n",
    "\n",
    "# Plot output\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(x_vec, y_vec, label='Network output')\n",
    "#show the points from the test dataset\n",
    "plt.scatter(test_df.input, test_df.label, label='Test points')\n",
    "\n",
    "#plt.plot(x_vec, true_model, label='True model')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement more stuff!!!!!!!!!!!\n",
    "Need to develop: advance optimizers and regularization methods (initialization scheme, momentum, ADAM, early stopping, L1, L2, sparsity dropuut)\n",
    "\n",
    "Also hyperparameters tuning is required, like learning rate (see if there are others)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ce988420084d6967f6f691e92dbd29d25bae6b884c44537ba941a79cb2b89434"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
