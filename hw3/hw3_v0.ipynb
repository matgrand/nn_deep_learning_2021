{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOMEWORK 3 NEURAL NETWORKS AND DEEP LEARNING\n",
    "\n",
    "---\n",
    "A.A. 2021/22 (6 CFU) - Dr. Alberto Testolin, Dr. Umberto Michieli\n",
    "---\n",
    "Student: Matteo Grandin\n",
    "---\n",
    "id: 2020374"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforced Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General overview\n",
    "In this homework you will learn how to implement and test neural network models for\n",
    "solving reinforcement learning problems. The basic tasks for the homework will require to implement some\n",
    "extensions to the code that you have seen in the Lab. The advanced tasks will require to train and test your\n",
    "learning agent on a different type of input (image pixels) or Gym environment. You can just choose one of\n",
    "the advanced tasks to get the maximum grade. If you are interested in improving your skills, feel free to try\n",
    "both advanced tasks. Given the higher computational complexity of RL, in this homework you don’t need to\n",
    "tune learning hyperparameters using search procedures and cross-validation; however, you are encouraged\n",
    "to play with model hyperparameters to find a satisfactory configuration.\n",
    "\n",
    "\n",
    "- 3 pt: use the notebook of Lab 07 to study how the exploration profile (either using eps-greedy or\n",
    "softmax) impacts the learning curve. Tune a bit the model hyperparameters or tweak the reward\n",
    "function to speed-up learning convergence (i.e., reach the same accuracy with fewer training episodes).\n",
    "\n",
    "YOU CAN DO JUST 1 OR BOTH\n",
    "- 5 pt: extend the notebook used in Lab 07, in order to learn to control the CartPole environment using\n",
    "directly the screen pixels, rather than the compact state representation used during the Lab (cart\n",
    "position, cart velocity, pole angle, pole angular velocity). NB: this will require to change the\n",
    "“observation_space” and to look for smart ways of encoding the pixels in a compact way to reduce\n",
    "computational complexity (e.g., crop the image around the pole, use difference of consecutive frames\n",
    "as input to consider temporal context, etc.).\n",
    "\n",
    "OR\n",
    "\n",
    "- 5 pt: train a deep RL agent on a different Gym environment. You are free to choose whatever Gym\n",
    "environment you like from the available list, or even explore other simulation platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jBelL8nBZVsn"
   },
   "outputs": [],
   "source": [
    "!pip install gym\n",
    "!apt update\n",
    "!apt-get install python-opengl -y\n",
    "!apt install xvfb -y\n",
    "!pip install pyvirtualdisplay\n",
    "!pip install piglet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a7EkHQ0VsNnJ"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from torch import nn\n",
    "from collections import deque # this python module implements exactly what we need for the replay memeory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E0oXbzI2ZAfv"
   },
   "outputs": [],
   "source": [
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yr5j5rz0ZEJf"
   },
   "outputs": [],
   "source": [
    "# This code creates a virtual display to draw game images on. \n",
    "# If you are running locally, just ignore it\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "StaxWEA0Z-O1"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Utility functions to enable video recording of gym environment and displaying it\n",
    "To enable video, just do \"env = wrap_env(env)\"\"\n",
    "\"\"\"\n",
    "\n",
    "def show_videos():\n",
    "  mp4list = glob.glob('video/*.mp4')\n",
    "  mp4list.sort()\n",
    "  for mp4 in mp4list:\n",
    "    print(f\"\\nSHOWING VIDEO {mp4}\")\n",
    "    video = io.open(mp4, 'r+b').read()\n",
    "    encoded = base64.b64encode(video)\n",
    "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "    \n",
    "def wrap_env(env, video_callable=None):\n",
    "  env = Monitor(env, './video', force=True, video_callable=video_callable)\n",
    "  return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1axKWEBl8uik"
   },
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity) # Define a queue with maxlen \"capacity\"\n",
    "\n",
    "    def push(self, state, action, next_state, reward):\n",
    "        # TODO: Add the tuple (state, action, next_state, reward) to the queue\n",
    "        self.memory.append((state, action, next_state, reward))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch_size = min(batch_size, len(self)) # Get all the samples if the requested batch_size is higher than the number of sample currently in the memory\n",
    "        return random.sample(self.memory, batch_size) # Randomly select \"batch_size\" samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory) # Return the number of samples currently stored in the memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W41rXekb8x7K"
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, state_space_dim, action_space_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            #inpt layer\n",
    "            nn.Linear(state_space_dim, 128),\n",
    "            nn.Tanh(),\n",
    "            #hidden layer\n",
    "            nn.Linear(128, 128),\n",
    "            nn.Tanh(),\n",
    "            #outpt layer\n",
    "            nn.Linear(128, action_space_dim),\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tLPAsTT4gpeW"
   },
   "outputs": [],
   "source": [
    "# Define an example network\n",
    "net = DQN(state_space_dim=4, action_space_dim=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ynfEXqjGfbpl"
   },
   "source": [
    "## Exploration Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZXfh_Ub1fv4c"
   },
   "outputs": [],
   "source": [
    "def choose_action_epsilon_greedy(net, state, epsilon):\n",
    "    \n",
    "    if epsilon > 1 or epsilon < 0:\n",
    "        raise Exception('The epsilon value must be between 0 and 1')\n",
    "                \n",
    "    # Evaluate the network output from the current state\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        state = torch.tensor(state, dtype=torch.float32) # Convert the state to tensor\n",
    "        net_out = net(state)\n",
    "\n",
    "    # Get the best action (argmax of the network output)\n",
    "    best_action = int(net_out.argmax())\n",
    "    # Get the number of possible actions\n",
    "    action_space_dim = net_out.shape[-1]\n",
    "\n",
    "    # Select a non optimal action with probability epsilon, otherwise choose the best action\n",
    "    if random.random() < epsilon:\n",
    "        # List of non-optimal actions\n",
    "        non_optimal_actions = [a for a in range(action_space_dim) if a != best_action]\n",
    "        # Select randomly\n",
    "        action = random.choice(non_optimal_actions)\n",
    "    else:\n",
    "        # Select best action\n",
    "        action = best_action\n",
    "        \n",
    "    return action, net_out.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "taW_cjBsf4sW"
   },
   "outputs": [],
   "source": [
    "def choose_action_softmax(net, state, temperature):\n",
    "    \n",
    "    if temperature < 0:\n",
    "        raise Exception('The temperature value must be greater than or equal to 0 ')\n",
    "        \n",
    "    # If the temperature is 0, just select the best action using the eps-greedy policy with epsilon = 0\n",
    "    if temperature == 0:\n",
    "        return choose_action_epsilon_greedy(net, state, 0)\n",
    "    \n",
    "    # Evaluate the network output from the current state\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "        net_out = net(state)\n",
    "\n",
    "    # Apply softmax with temp\n",
    "    temperature = max(temperature, 1e-8) # set a minimum to the temperature for numerical stability\n",
    "    softmax_out = nn.functional.softmax(net_out / temperature, dim=0).numpy()\n",
    "                \n",
    "    # Sample the action using softmax output as mass pdf\n",
    "    all_possible_actions = np.arange(0, softmax_out.shape[-1])\n",
    "    action = np.random.choice(all_possible_actions, p=softmax_out) # this samples a random element from \"all_possible_actions\" with the probability distribution p (softmax_out in this case)\n",
    "    \n",
    "    return action, net_out.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QMXhJfZcpqIA"
   },
   "source": [
    "### Exploration profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G-f78CC3ptt2"
   },
   "outputs": [],
   "source": [
    "### Define exploration profile\n",
    "initial_value = 5\n",
    "num_iterations = 300 #1000\n",
    "exp_decay = np.exp(-np.log(initial_value) / num_iterations * 6) # We compute the exponential decay in such a way the shape of the exploration profile does not depend on the number of iterations\n",
    "exploration_profile = [initial_value * (exp_decay ** i) for i in range(num_iterations)]\n",
    "\n",
    "### Plot exploration profile\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(exploration_profile)\n",
    "plt.grid()\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Exploration profile (Softmax temperature)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zjKmdFhowRbj"
   },
   "source": [
    "# Gym Environment (CartPole-v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ar0yNaNrCnjn"
   },
   "outputs": [],
   "source": [
    "### Create environment\n",
    "env = gym.make('CartPole-v1') # Initialize the Gym environment\n",
    "env.seed(0) # Set a random seed for the environment (reproducible results)\n",
    "\n",
    "# Get the shapes of the state space (observation_space) and action space (action_space)\n",
    "state_space_dim = env.observation_space.shape[0]\n",
    "action_space_dim = env.action_space.n\n",
    "\n",
    "print(f\"STATE SPACE SIZE: {state_space_dim}\")\n",
    "print(f\"ACTION SPACE SIZE: {action_space_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2g4NatpgSpbc"
   },
   "outputs": [],
   "source": [
    "# Initialize the Gym environment\n",
    "env = gym.make('CartPole-v1') \n",
    "env.seed(0) # Set a random seed for the environment (reproducible results)\n",
    "\n",
    "# This is for creating the output video in Colab, not required outside Colab\n",
    "env = wrap_env(env, video_callable=lambda episode_id: True)\n",
    "\n",
    "# Let's try for a total of 10 episodes\n",
    "for num_episode in range(10): \n",
    "    # Reset the environment and get the initial state\n",
    "    state = env.reset()\n",
    "    # Reset the score. The final score will be the total amount of steps before the pole falls\n",
    "    score = 0\n",
    "    done = False\n",
    "    # Go on until the pole falls off or the score reach 490\n",
    "    while not done and score < 490:\n",
    "      # Choose a random action\n",
    "      action = random.choice([0, 1])\n",
    "      # Apply the action and get the next state, the reward and a flag \"done\" that is True if the game is ended\n",
    "      next_state, reward, done, info = env.step(action)\n",
    "      # Visually render the environment (optional, comment this line to speed up the simulation)\n",
    "      env.render()\n",
    "      # Update the final score (+1 for each step)\n",
    "      score += reward \n",
    "      # Set the current state for the next iteration\n",
    "      state = next_state\n",
    "      # Check if the episode ended (the pole fell down)\n",
    "    # Print the final score\n",
    "    print(f\"EPISODE {num_episode + 1} - FINAL SCORE: {score}\") \n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AnX-nFLiVPH6"
   },
   "outputs": [],
   "source": [
    "# Display the videos, not required outside Colab\n",
    "show_videos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9KXpjzf2vdeL"
   },
   "source": [
    "# Network update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lVpk-g0i9d-B"
   },
   "outputs": [],
   "source": [
    "# Set random seeds\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "### PARAMETERS\n",
    "gamma = 0.97   # gamma parameter for the long term reward\n",
    "replay_memory_capacity = 10000   # Replay memory capacity\n",
    "lr = 1e-2   # Optimizer learning rate\n",
    "target_net_update_steps = 10   # Number of episodes to wait before updating the target network\n",
    "batch_size = 128   # Number of samples to take from the replay memory for each update\n",
    "bad_state_penalty = 0   # Penalty to the reward when we are in a bad state (in this case when the pole falls down) \n",
    "min_samples_for_training = 1000   # Minimum samples in the replay memory to enable the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2k3ZQuh8xHo7"
   },
   "outputs": [],
   "source": [
    "### Initialize the replay memory\n",
    "replay_mem = ReplayMemory(replay_memory_capacity)    \n",
    "\n",
    "### Initialize the policy network\n",
    "policy_net = DQN(state_space_dim, action_space_dim)\n",
    "\n",
    "### Initialize the target network with the same weights of the policy network\n",
    "target_net = DQN(state_space_dim, action_space_dim)\n",
    "target_net.load_state_dict(policy_net.state_dict()) # This will copy the weights of the policy network to the target network\n",
    "\n",
    "### Initialize the optimizer\n",
    "optimizer = torch.optim.SGD(policy_net.parameters(), lr=lr) # The optimizer will update ONLY the parameters of the policy network\n",
    "\n",
    "### Initialize the loss function (Huber loss)\n",
    "loss_fn = nn.SmoothL1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sj1hEvPOvkBX"
   },
   "outputs": [],
   "source": [
    "def update_step(policy_net, target_net, replay_mem, gamma, optimizer, loss_fn, batch_size):\n",
    "        \n",
    "    # Sample the data from the replay memory\n",
    "    batch = replay_mem.sample(batch_size)\n",
    "    batch_size = len(batch)\n",
    "\n",
    "    # Create tensors for each element of the batch\n",
    "    states      = torch.tensor([s[0] for s in batch], dtype=torch.float32)\n",
    "    actions     = torch.tensor([s[1] for s in batch], dtype=torch.int64)\n",
    "    rewards     = torch.tensor([s[3] for s in batch], dtype=torch.float32)\n",
    "\n",
    "    # Compute a mask of non-final states (all the elements where the next state is not None)\n",
    "    non_final_next_states = torch.tensor([s[2] for s in batch if s[2] is not None], dtype=torch.float32) # the next state can be None if the game has ended\n",
    "    non_final_mask = torch.tensor([s[2] is not None for s in batch], dtype=torch.bool)\n",
    "\n",
    "    # Compute all the Q values (forward pass)\n",
    "    policy_net.train()\n",
    "    q_values = policy_net(states)\n",
    "    # Select the proper Q value for the corresponding action taken Q(s_t, a)\n",
    "    state_action_values = q_values.gather(1, actions.unsqueeze(1))\n",
    "\n",
    "    # Compute the value function of the next states using the target network V(s_{t+1}) = max_a( Q_target(s_{t+1}, a)) )\n",
    "    with torch.no_grad():\n",
    "      target_net.eval()\n",
    "      q_values_target = target_net(non_final_next_states)\n",
    "    next_state_max_q_values = torch.zeros(batch_size)\n",
    "    next_state_max_q_values[non_final_mask] = q_values_target.max(dim=1)[0]\n",
    "\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = rewards + (next_state_max_q_values * gamma)\n",
    "    expected_state_action_values = expected_state_action_values.unsqueeze(1) # Set the required tensor shape\n",
    "\n",
    "    # Compute the Huber loss\n",
    "    loss = loss_fn(state_action_values, expected_state_action_values)\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # Apply gradient clipping (clip all the gradients greater than 2 for training stability)\n",
    "    nn.utils.clip_grad_norm_(policy_net.parameters(), 2)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WF6Zf53FoRDZ"
   },
   "outputs": [],
   "source": [
    "# Initialize the Gym environment\n",
    "env = gym.make('CartPole-v1') \n",
    "env.seed(0) # Set a random seed for the environment (reproducible results)\n",
    "\n",
    "# This is for creating the output video in Colab, not required outside Colab\n",
    "env = wrap_env(env, video_callable=lambda episode_id: episode_id % 100 == 0) # Save a video every 100 episodes\n",
    "\n",
    "for episode_num, tau in enumerate(tqdm(exploration_profile)):\n",
    "\n",
    "    # Reset the environment and get the initial state\n",
    "    state = env.reset()\n",
    "    # Reset the score. The final score will be the total amount of steps before the pole falls\n",
    "    score = 0\n",
    "    done = False\n",
    "\n",
    "    # Go on until the pole falls off\n",
    "    while not done:\n",
    "\n",
    "      # Choose the action following the policy\n",
    "      action, q_values = choose_action_softmax(policy_net, state, temperature=tau)\n",
    "      \n",
    "      # Apply the action and get the next state, the reward and a flag \"done\" that is True if the game is ended\n",
    "      next_state, reward, done, info = env.step(action)\n",
    "\n",
    "      # We apply a (linear) penalty when the cart is far from center\n",
    "      pos_weight = 1\n",
    "      reward = reward - pos_weight * np.abs(state[0]) \n",
    "\n",
    "      # Update the final score (+1 for each step)\n",
    "      score += 1\n",
    "\n",
    "      # Apply penalty for bad state\n",
    "      if done: # if the pole has fallen down \n",
    "          reward += bad_state_penalty\n",
    "          next_state = None\n",
    "\n",
    "      # Update the replay memory\n",
    "      replay_mem.push(state, action, next_state, reward)\n",
    "\n",
    "      # Update the network\n",
    "      if len(replay_mem) > min_samples_for_training: # we enable the training only if we have enough samples in the replay memory, otherwise the training will use the same samples too often\n",
    "          update_step(policy_net, target_net, replay_mem, gamma, optimizer, loss_fn, batch_size)\n",
    "\n",
    "      # Visually render the environment (disable to speed up the training)\n",
    "      env.render()\n",
    "\n",
    "      # Set the current state for the next iteration\n",
    "      state = next_state\n",
    "\n",
    "    # Update the target network every target_net_update_steps episodes\n",
    "    if episode_num % target_net_update_steps == 0:\n",
    "        print('Updating target network...')\n",
    "        target_net.load_state_dict(policy_net.state_dict()) # This will copy the weights of the policy network to the target network\n",
    "\n",
    "    # Print the final score\n",
    "    print(f\"EPISODE: {episode_num + 1} - FINAL SCORE: {score} - Temperature: {tau}\") # Print the final score\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7FQjjVTiP2T6"
   },
   "outputs": [],
   "source": [
    "# Display the videos, not required outside Colab\n",
    "show_videos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JkG9iDZTIhzc"
   },
   "source": [
    "# Final test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vJKgnu3_IjWE"
   },
   "outputs": [],
   "source": [
    "# Initialize the Gym environment\n",
    "env = gym.make('CartPole-v1') \n",
    "env.seed(1) # Set a random seed for the environment (reproducible results)\n",
    "\n",
    "# This is for creating the output video in Colab, not required outside Colab\n",
    "env = wrap_env(env, video_callable=lambda episode_id: True) # Save a video every episode\n",
    "\n",
    "# Let's try for a total of 10 episodes\n",
    "for num_episode in range(10): \n",
    "    # Reset the environment and get the initial state\n",
    "    state = env.reset()\n",
    "    # Reset the score. The final score will be the total amount of steps before the pole falls\n",
    "    score = 0\n",
    "    done = False\n",
    "    # Go on until the pole falls off or the score reach 490\n",
    "    while not done:\n",
    "      # Choose the best action (temperature 0)\n",
    "      action, q_values = choose_action_softmax(policy_net, state, temperature=0)\n",
    "      # Apply the action and get the next state, the reward and a flag \"done\" that is True if the game is ended\n",
    "      next_state, reward, done, info = env.step(action)\n",
    "      # Visually render the environment\n",
    "      env.render()\n",
    "      # Update the final score (+1 for each step)\n",
    "      score += reward \n",
    "      # Set the current state for the next iteration\n",
    "      state = next_state\n",
    "      # Check if the episode ended (the pole fell down)\n",
    "    # Print the final score\n",
    "    print(f\"EPISODE {num_episode + 1} - FINAL SCORE: {score}\") \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mxdtGJutLqlw"
   },
   "outputs": [],
   "source": [
    "# Display the videos, not required outside Colab\n",
    "show_videos()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cee89b7c6bc96453738565335b56b694d8a30ac65e979633b683f8408c8233c6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('dl_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
