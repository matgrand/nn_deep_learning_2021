{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOMEWORK 3 NEURAL NETWORKS AND DEEP LEARNING\n",
    "\n",
    "---\n",
    "A.A. 2021/22 (6 CFU) - Dr. Alberto Testolin, Dr. Umberto Michieli\n",
    "---\n",
    "Student: Matteo Grandin\n",
    "---\n",
    "id: 2020374"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforced Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General overview\n",
    "In this homework you will learn how to implement and test neural network models for\n",
    "solving reinforcement learning problems. The basic tasks for the homework will require to implement some\n",
    "extensions to the code that you have seen in the Lab. The advanced tasks will require to train and test your\n",
    "learning agent on a different type of input (image pixels) or Gym environment. You can just choose one of\n",
    "the advanced tasks to get the maximum grade. If you are interested in improving your skills, feel free to try\n",
    "both advanced tasks. Given the higher computational complexity of RL, in this homework you don’t need to\n",
    "tune learning hyperparameters using search procedures and cross-validation; however, you are encouraged\n",
    "to play with model hyperparameters to find a satisfactory configuration.\n",
    "\n",
    "\n",
    "- 3 pt: use the notebook of Lab 07 to study how the exploration profile (either using eps-greedy or\n",
    "softmax) impacts the learning curve. Tune a bit the model hyperparameters or tweak the reward\n",
    "function to speed-up learning convergence (i.e., reach the same accuracy with fewer training episodes).\n",
    "\n",
    "YOU CAN DO JUST 1 OR BOTH\n",
    "- 5 pt: extend the notebook used in Lab 07, in order to learn to control the CartPole environment using\n",
    "directly the screen pixels, rather than the compact state representation used during the Lab (cart\n",
    "position, cart velocity, pole angle, pole angular velocity). NB: this will require to change the\n",
    "“observation_space” and to look for smart ways of encoding the pixels in a compact way to reduce\n",
    "computational complexity (e.g., crop the image around the pole, use difference of consecutive frames\n",
    "as input to consider temporal context, etc.).\n",
    "\n",
    "OR\n",
    "\n",
    "- 5 pt: train a deep RL agent on a different Gym environment. You are free to choose whatever Gym\n",
    "environment you like from the available list, or even explore other simulation platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean value to check if we are in colab or not\n",
    "colab = False \n",
    "show_video = False\n",
    "show_render = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jBelL8nBZVsn"
   },
   "outputs": [],
   "source": [
    "if colab:\n",
    "    !pip install gym\n",
    "else:\n",
    "    %pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a7EkHQ0VsNnJ"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from torch import nn\n",
    "from collections import deque # this python module implements exactly what we need for the replay memeory\n",
    "\n",
    "#use gpu if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E0oXbzI2ZAfv"
   },
   "outputs": [],
   "source": [
    "if colab:\n",
    "    !apt update\n",
    "    !apt-get install python-opengl -y\n",
    "    !apt install xvfb -y\n",
    "    !pip install pyvirtualdisplay\n",
    "    !pip install piglet\n",
    "    import glob\n",
    "    import io\n",
    "    import base64\n",
    "    import os\n",
    "    from IPython.display import HTML\n",
    "    from IPython import display as ipythondisplay\n",
    "    from pyvirtualdisplay import Display\n",
    "    from gym.wrappers import Monitor\n",
    "\n",
    "    display = Display(visible=0, size=(1400, 900))\n",
    "    display.start()\n",
    "\n",
    "    if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
    "        !bash ../xvfb start\n",
    "        %env DISPLAY=:1\n",
    "\n",
    "    \"\"\"\n",
    "    Utility functions to enable video recording of gym environment and displaying it\n",
    "    To enable video, just do \"env = wrap_env(env)\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    def show_videos():\n",
    "        mp4list = glob.glob('video/*.mp4')\n",
    "        mp4list.sort()\n",
    "        for mp4 in mp4list:\n",
    "            print(f\"\\nSHOWING VIDEO {mp4}\")\n",
    "            video = io.open(mp4, 'r+b').read()\n",
    "            encoded = base64.b64encode(video)\n",
    "            ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                        loop controls style=\"height: 400px;\">\n",
    "                        <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "                    </video>'''.format(encoded.decode('ascii'))))\n",
    "            \n",
    "    def wrap_env(env, video_callable=None):\n",
    "        env = Monitor(env, './video', force=True, video_callable=video_callable)\n",
    "        return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1axKWEBl8uik"
   },
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity, state_size=2):\n",
    "        #capacity, state, action, reward, next_state, mask\n",
    "        self.memory = torch.zeros((capacity, 2*state_size+1+1+1), dtype=torch.float32, device=device) #capacity, action, reward, 2 states\n",
    "        self.state_size = state_size\n",
    "        self.memory_index = 0\n",
    "        self.capacity = capacity\n",
    "        self.full = False\n",
    "\n",
    "    def push(self, state, action, next_state, reward):\n",
    "        if state is None or action is None or reward is None:\n",
    "            print(\"WARNING: None value passed to ReplayMemory.push()\")\n",
    "            return\n",
    "        # TODO: Add the tuple (state, action, next_state, reward) to the queue\n",
    "        s = torch.from_numpy(state).float().to(device)\n",
    "        a = torch.tensor([action], dtype=torch.float32, device=device)\n",
    "        r = torch.tensor([reward], dtype=torch.float32, device=device)\n",
    "        if next_state is not None:\n",
    "            next_s = torch.from_numpy(next_state).float().to(device) \n",
    "            mask = torch.tensor([1], dtype=torch.float32, device=device)\n",
    "        else:\n",
    "            next_s = torch.zeros((self.state_size), dtype=torch.float32, device=device)\n",
    "            mask = torch.tensor([0], dtype=torch.float32, device=device) # mask is false\n",
    "        \n",
    "        to_add = torch.cat((s, a, r, next_s, mask), dim=0 )\n",
    "        #assert to_add.shape == (1, 2*self.state_size+1+1+1),  f\"to_add: {to_add.shape}, \\n {to_add}\"\n",
    "        self.memory[self.memory_index, :] = to_add\n",
    "\n",
    "        #increase the index, and wrap it around if it is bigger than the capacity\n",
    "        if self.memory_index == self.capacity-1:\n",
    "            self.full = True\n",
    "        self.memory_index = (self.memory_index + 1) % self.memory.shape[0]\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        batch_size = min(batch_size, len(self)) # Get all the samples if the requested batch_size is higher than the number of sample currently in the memory\n",
    "        # select indeces\n",
    "        pool = self.capacity if self.full else self.memory_index-1\n",
    "        indeces = np.random.choice(pool, batch_size, replace=False)\n",
    "        # return the samples\n",
    "        ret = self.memory[indeces, :]\n",
    "        states = ret[:, 0:self.state_size].unsqueeze(1)\n",
    "        actions = ret[:, self.state_size].long()\n",
    "        rewards = ret[:, self.state_size+1].float()\n",
    "        non_final_mask = ret[:, self.state_size+2].bool()\n",
    "        non_final_next_states = ret[:, self.state_size+3:].unsqueeze(1)\n",
    "        non_final_next_states = non_final_next_states[non_final_mask]\n",
    "\n",
    "        #print(f\"states: {states.shape}, actions: {actions.shape}, rewards: {rewards.shape}, non_final_next_states: {non_final_next_states.shape}, non_final_mask: {non_final_mask.shape}\")\n",
    "        return states, actions, rewards, non_final_next_states, non_final_mask\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.full:\n",
    "            return self.capacity\n",
    "        else:\n",
    "            return self.memory_index\n",
    "        # return len(self.memory) # Return the number of samples currently stored in the memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W41rXekb8x7K"
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, state_space_dim, action_space_dim):\n",
    "        super().__init__()\n",
    "        n_hid = 128\n",
    "        self.linear = nn.Sequential(\n",
    "            #inpt layer\n",
    "            nn.Linear(state_space_dim, n_hid),\n",
    "            nn.Tanh(),\n",
    "            #nn.Dropout(0.2), #bad\n",
    "            #hidden layer\n",
    "            nn.Linear(n_hid, n_hid),\n",
    "            nn.Tanh(),\n",
    "            #nn.Dropout(0.2), #bad\n",
    "            #outpt layer\n",
    "            nn.Linear(n_hid, action_space_dim),\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ynfEXqjGfbpl"
   },
   "source": [
    "## Exploration Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZXfh_Ub1fv4c"
   },
   "outputs": [],
   "source": [
    "def choose_action_epsilon_greedy(net, state, epsilon):\n",
    "    if epsilon > 1 or epsilon < 0:\n",
    "        raise Exception('The epsilon value must be between 0 and 1')\n",
    "                \n",
    "    # Evaluate the network output from the current state\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        state = torch.tensor(state, dtype=torch.float32,device=device) # Convert the state to tensor\n",
    "        net_out = net(state)\n",
    "\n",
    "    # Get the best action (argmax of the network output)\n",
    "    best_action = int(net_out.argmax())\n",
    "    # Get the number of possible actions\n",
    "    action_space_dim = net_out.shape[-1]\n",
    "\n",
    "    # Select a non optimal action with probability epsilon, otherwise choose the best action\n",
    "    if random.random() < epsilon:\n",
    "        # List of non-optimal actions\n",
    "        non_optimal_actions = [a for a in range(action_space_dim) if a != best_action]\n",
    "        # Select randomly\n",
    "        action = random.choice(non_optimal_actions)\n",
    "    else:\n",
    "        # Select best action\n",
    "        action = best_action\n",
    "        \n",
    "    return action, net_out.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "taW_cjBsf4sW"
   },
   "outputs": [],
   "source": [
    "def choose_action_softmax(net, state, temperature):\n",
    "    if temperature < 0:\n",
    "        raise Exception('The temperature value must be greater than or equal to 0 ')\n",
    "        \n",
    "    # If the temperature is 0, just select the best action using the eps-greedy policy with epsilon = 0\n",
    "    if temperature == 0:\n",
    "        return choose_action_epsilon_greedy(net, state, 0)\n",
    "    \n",
    "    # Evaluate the network output from the current state\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=device)\n",
    "        net_out = net(state)\n",
    "\n",
    "    # Apply softmax with temp\n",
    "    temperature = max(temperature, 1e-8) # set a minimum to the temperature for numerical stability\n",
    "    softmax_out = nn.functional.softmax(net_out / temperature, dim=0).cpu().numpy()\n",
    "                \n",
    "    # Sample the action using softmax output as mass pdf\n",
    "    all_possible_actions = np.arange(0, softmax_out.shape[-1])\n",
    "    action = np.random.choice(all_possible_actions, p=softmax_out) # this samples a random element from \"all_possible_actions\" with the probability distribution p (softmax_out in this case)\n",
    "    \n",
    "    return action, net_out.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QMXhJfZcpqIA"
   },
   "source": [
    "### Exploration profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G-f78CC3ptt2"
   },
   "outputs": [],
   "source": [
    "### Define exploration profile\n",
    "def get_exploration_profile(initial_value=0.5, num_iterations=1000, speed = 3): # speed = speed of convergence, bigger is faster to reach 0\n",
    "    if initial_value < 0:\n",
    "        raise Exception('The initial value must be > 0')\n",
    "    if initial_value > 1:\n",
    "        exp_decay = np.exp(-np.log(initial_value) / num_iterations * speed) # We compute the exponential decay in such a way the shape of the exploration profile does not depend on the number of iterations\n",
    "    else :\n",
    "        exp_decay = np.exp(np.log(initial_value) / num_iterations * speed)\n",
    "    exploration_profile = [initial_value * (exp_decay ** i) for i in range(num_iterations)]\n",
    "    return exploration_profile\n",
    "\n",
    "exploration_profile = get_exploration_profile()\n",
    "\n",
    "### Plot exploration profile\n",
    "plt.figure(figsize=(12,3))\n",
    "plt.plot(exploration_profile)\n",
    "plt.grid()\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Exploration profile (Softmax temperature)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zjKmdFhowRbj"
   },
   "source": [
    "# Gym Environment ('MountainCar-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ar0yNaNrCnjn"
   },
   "outputs": [],
   "source": [
    "### Create environment\n",
    "chosen_env = 'MountainCar-v0'\n",
    "env = gym.make(chosen_env) # Initialize the Gym environment\n",
    "env.seed(0) # Set a random seed for the environment (reproducible results)\n",
    "\n",
    "# Get the shapes of the state space (observation_space) and action space (action_space)\n",
    "state_space_dim = env.observation_space.shape[0]\n",
    "action_space_dim = env.action_space.n\n",
    "print(f\"state_space_dim: {state_space_dim}, action_space_dim: {action_space_dim}\")\n",
    "\n",
    "if colab:\n",
    "  env = wrap_env(env, video_callable=lambda episode_id: True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9KXpjzf2vdeL"
   },
   "source": [
    "# Network update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lVpk-g0i9d-B"
   },
   "outputs": [],
   "source": [
    "# Set random seeds\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "### PARAMETERS\n",
    "gamma = 0.97  #0.97 gamma parameter for the long term reward \n",
    "replay_memory_capacity = 20000   #10000 Replay memory capacity \n",
    "lr = 2e-2   #1e-2 Optimizer learning rate \n",
    "target_net_update_steps = 5   #10 Number of episodes to wait before updating the target network \n",
    "batch_size = 4096   #128 Number of samples to take from the replay memory for each update \n",
    "bad_state_penalty = 0   #0 Penalty to the reward when we are in a bad state (in this case when the pole falls down)\n",
    "min_samples_for_training = 5000   #1000 Minimum samples in the replay memory to enable the training \n",
    "use_epsilon_greedy = False   #False Use epsilon greedy exploration or not \n",
    "#set up exploration profile for softmax\n",
    "initial_value = 4 #5 Initial temperature/epsilon value for the exploration profile \n",
    "num_iterations = 300 # Number of episodes 1000, \n",
    "convergence_speed = 6 #6 Speed of convergence of the exploration profile  \n",
    "exploration_profile = get_exploration_profile(initial_value, num_iterations, speed = convergence_speed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lowering target_net_update_steps from 10 to 8 had a huge impact, learning rate could be set higher too and gamma could be lowered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2k3ZQuh8xHo7"
   },
   "outputs": [],
   "source": [
    "### Initialize the replay memory\n",
    "replay_mem = ReplayMemory(replay_memory_capacity)    \n",
    "\n",
    "### Initialize the policy network\n",
    "policy_net = DQN(state_space_dim, action_space_dim).to(device)\n",
    "\n",
    "### Initialize the target network with the same weights of the policy network\n",
    "target_net = DQN(state_space_dim, action_space_dim).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict()) # This will copy the weights of the policy network to the target network\n",
    "\n",
    "### Initialize the optimizer\n",
    "optimizer = torch.optim.Adam(policy_net.parameters(), lr=lr) # The optimizer will update ONLY the parameters of the policy network\n",
    "\n",
    "### Initialize the loss function (Huber loss)\n",
    "loss_fn = nn.SmoothL1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sj1hEvPOvkBX"
   },
   "outputs": [],
   "source": [
    "def update_step(policy_net, target_net, replay_mem, gamma, optimizer, loss_fn, batch_size):\n",
    "        \n",
    "  # Sample the data from the replay memory\n",
    "  states, actions, rewards, non_final_next_states, non_final_mask = replay_mem.sample(batch_size)\n",
    "\n",
    "  #print(f\"states: {states.shape}, actions: {actions.shape}, rewards: {rewards.shape}, non_final_next_states: {non_final_next_states.shape}, non_final_mask: {non_final_mask.shape}\")\n",
    "\n",
    "  # Compute all the Q values (forward pass)\n",
    "  policy_net.train()\n",
    "  q_values = policy_net(states).squeeze(1)\n",
    "\n",
    "  # Select the proper Q value for the corresponding action taken Q(s_t, a)\n",
    "  #print(f\"q_values: {q_values.shape}, actions: {actions}\")\n",
    "  state_action_values = q_values.gather(1, actions.unsqueeze(1))\n",
    "\n",
    "  # Compute the value function of the next states using the target network V(s_{t+1}) = max_a( Q_target(s_{t+1}, a)) )\n",
    "  with torch.no_grad():\n",
    "    target_net.eval()\n",
    "    q_values_target = target_net(non_final_next_states).squeeze(1)\n",
    "  next_state_max_q_values = torch.zeros(batch_size, device=device)\n",
    "  #print(f\"q_values_target: {q_values_target.shape}, non_final_mask: {non_final_mask.shape}, next_state_max_q_values: {next_state_max_q_values.shape}\")\n",
    "  next_state_max_q_values[non_final_mask] = q_values_target.max(dim=1)[0]\n",
    "\n",
    "  # Compute the expected Q values\n",
    "  expected_state_action_values = rewards + (next_state_max_q_values * gamma)\n",
    "  expected_state_action_values = expected_state_action_values.unsqueeze(1) # Set the required tensor shape\n",
    "\n",
    "  # Compute the Huber loss\n",
    "  loss = loss_fn(state_action_values, expected_state_action_values)\n",
    "\n",
    "  # Optimize the model\n",
    "  optimizer.zero_grad()\n",
    "  loss.backward()\n",
    "  # Apply gradient clipping (clip all the gradients greater than 2 for training stability)\n",
    "  nn.utils.clip_grad_norm_(policy_net.parameters(), 2)\n",
    "  optimizer.step()\n",
    "  return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WF6Zf53FoRDZ"
   },
   "outputs": [],
   "source": [
    "# Initialize the Gym environment\n",
    "env = gym.make(chosen_env) \n",
    "env.seed(0) # Set a random seed for the environment (reproducible results)\n",
    "\n",
    "# This is for creating the output video in Colab, not required outside Colab\n",
    "if colab:\n",
    "    env = wrap_env(env, video_callable=lambda episode_id: episode_id % 100 == 0) # Save a video every 100 episodes\n",
    "\n",
    "#list of losses and scores\n",
    "losses, scores = [],[]\n",
    "for episode_num, tau in enumerate(tqdm(exploration_profile)):\n",
    "    if use_epsilon_greedy:\n",
    "        epsilon = tau\n",
    "\n",
    "    # Reset the environment and get the initial state\n",
    "    state = env.reset()\n",
    "    # Reset the score, score will be the maximum position reached\n",
    "    position = -0.5\n",
    "    max_speed = 0\n",
    "    done = False\n",
    "    score = -1\n",
    "    episode_loss = []\n",
    "    tot_reward= 0\n",
    "    while not done:\n",
    "        # Choose the action following the policy\n",
    "        action, q_values = choose_action_epsilon_greedy(policy_net, state, epsilon) if use_epsilon_greedy else choose_action_softmax(policy_net, state, temperature=tau)\n",
    "        \n",
    "        # Apply the action and get the next state, the reward and a flag \"done\" that is True if the game is ended\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        ## SCORE UPDATE AND REWARD UPDATE\n",
    "        curr_position = -1\n",
    "        \n",
    "        if done: # more than 200 steps or car arrived at the end (>= 0.5)\n",
    "            next_state = None\n",
    "            curr_position = -1\n",
    "            curr_speed = -1\n",
    "            score = max(score, state[0]+1.2)\n",
    "        else:\n",
    "            curr_position = next_state[0]\n",
    "            curr_speed = next_state[1]\n",
    "\n",
    "        # big reward if it achieves the goal\n",
    "        if reward == 0:\n",
    "            reward += 200\n",
    "        else:\n",
    "            reward = -0.5\n",
    "\n",
    "        #reward if it beats it's previous score\n",
    "        if curr_position > position:\n",
    "            reward += 1 + 2*(curr_position - position)\n",
    "            position = curr_position\n",
    "\n",
    "        #reward if it achieves a higher speed\n",
    "        if curr_speed > max_speed:\n",
    "            reward += 1 + 1*(curr_speed - max_speed)\n",
    "            max_speed = curr_speed\n",
    "\n",
    "        # Update the replay memory\n",
    "        replay_mem.push(state, action, next_state, reward)\n",
    "\n",
    "        # Update the network\n",
    "        if len(replay_mem) > min_samples_for_training: # we enable the training only if we have enough samples in the replay memory, otherwise the training will use the same samples too often\n",
    "            loss = update_step(policy_net, target_net, replay_mem, gamma, optimizer, loss_fn, batch_size)\n",
    "            episode_loss.append(loss)\n",
    "            \n",
    "        # Visually render the environment (disable to speed up the training)\n",
    "        if not colab and show_render:\n",
    "            env.render()\n",
    "\n",
    "        # Set the current state for the next iteration\n",
    "        state = next_state\n",
    "        tot_reward += reward\n",
    "    #end of while loop\n",
    "\n",
    "    #get episode loss\n",
    "    average_episode_loss = np.mean(episode_loss)\n",
    "    losses.append(average_episode_loss)\n",
    "    #get episode score\n",
    "    scores.append(score)\n",
    "\n",
    "    # Update the target network every target_net_update_steps episodes\n",
    "    if episode_num % target_net_update_steps == 0:\n",
    "        #print('Updating target network...')\n",
    "        target_net.load_state_dict(policy_net.state_dict()) # This will copy the weights of the policy network to the target network\n",
    "\n",
    "\n",
    "\n",
    "    # Print the final score\n",
    "    print(f\"EPISODE: {episode_num + 1} - FINAL SCORE: {score:.2f} - Temperature/epsilon: {tau:.4f}, total reward: {tot_reward:.2f}\")\n",
    "\n",
    "    if episode_num > num_iterations*0.9:\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reward function is key, in this case the idea is that the agent get rewards only when it beats himself "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7FQjjVTiP2T6"
   },
   "outputs": [],
   "source": [
    "# Display the videos, not required outside Colab\n",
    "if colab and show_video:\n",
    "    show_videos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot scores and losses\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(scores)\n",
    "plt.title('Scores')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(losses)\n",
    "plt.title('Losses')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding an exploration profile makes the loss go down, however the score goes down with the loss too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JkG9iDZTIhzc"
   },
   "source": [
    "# Final test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vJKgnu3_IjWE"
   },
   "outputs": [],
   "source": [
    "# Initialize the Gym environment\n",
    "env = gym.make(chosen_env) \n",
    "env.seed(1) # Set a random seed for the environment (reproducible results)\n",
    "\n",
    "# This is for creating the output video in Colab, not required outside Colab\n",
    "if colab:\n",
    "  env = wrap_env(env, video_callable=lambda episode_id: True) # Save a video every episode\n",
    "\n",
    "show_render = True\n",
    "\n",
    "# Let's try for a total of 10 episodes\n",
    "scores = []\n",
    "for num_episode in range(10): \n",
    "    # Reset the environment and get the initial state\n",
    "    state = env.reset()\n",
    "    # Reset the score. The final score will be the total amount of steps before the pole falls\n",
    "    score = -0.5\n",
    "    done = False\n",
    "    # Go on until the pole falls off or the score reach 490\n",
    "    while not done:\n",
    "      # Choose the best action (temperature 0)\n",
    "      action, q_values = choose_action_epsilon_greedy(policy_net, state, epsilon=0) if use_epsilon_greedy else choose_action_softmax(policy_net, state, temperature=0) \n",
    "      # Apply the action and get the next state, the reward and a flag \"done\" that is True if the game is ended\n",
    "      next_state, reward, done, info = env.step(action)\n",
    "      # Visually render the environment\n",
    "      if not colab and show_render:\n",
    "        env.render()\n",
    "      # Update the final score (+1 for each step)\n",
    "      score = max(score, next_state[0]+1.2)\n",
    "      # Set the current state for the next iteration\n",
    "      state = next_state\n",
    "    # Print the final score\n",
    "    #print(f\"EPISODE {num_episode + 1} - FINAL SCORE: {score}\") \n",
    "    scores.append(score)  \n",
    "env.close()\n",
    "\n",
    "#print the average score\n",
    "print(f\"AVERAGE SCORE: {np.mean(scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let an agent play for a few episodes   \n",
    "#doesn't work in colab, but works in local\n",
    "if not colab:\n",
    "  env = gym.make(chosen_env)\n",
    "  #env.seed(42)\n",
    "\n",
    "  show_render = True\n",
    "\n",
    "  scores = []\n",
    "  # Reset the environment and get the initial state\n",
    "  state = env.reset()\n",
    "  # Reset the score. The final score will be the total amount of steps before the pole falls\n",
    "  # Go on\n",
    "  for i in range(1000):\n",
    "      # Choose the best action (temperature 0)\n",
    "      action, q_values = choose_action_epsilon_greedy(policy_net, state, epsilon=0) if use_epsilon_greedy else choose_action_softmax(policy_net, state, temperature=0) \n",
    "      # Apply the action and get the next state, the reward and a flag \"done\" that is True if the game is ended\n",
    "      next_state, reward, done, info = env.step(action)\n",
    "      # Visually render the environment\n",
    "      if not colab and show_render:\n",
    "          env.render()\n",
    "      # Set the current state for the next iteration\n",
    "      state = next_state\n",
    "  env.close()\n",
    "\n",
    "  show_render = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mxdtGJutLqlw"
   },
   "outputs": [],
   "source": [
    "# Display the videos, not required outside Colab\n",
    "if colab: \n",
    "    show_videos()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cee89b7c6bc96453738565335b56b694d8a30ac65e979633b683f8408c8233c6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('dl_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
