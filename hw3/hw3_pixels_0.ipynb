{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOMEWORK 3 NEURAL NETWORKS AND DEEP LEARNING\n",
    "\n",
    "---\n",
    "A.A. 2021/22 (6 CFU) - Dr. Alberto Testolin, Dr. Umberto Michieli\n",
    "---\n",
    "Student: Matteo Grandin\n",
    "---\n",
    "id: 2020374"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforced Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General overview\n",
    "In this homework you will learn how to implement and test neural network models for\n",
    "solving reinforcement learning problems. The basic tasks for the homework will require to implement some\n",
    "extensions to the code that you have seen in the Lab. The advanced tasks will require to train and test your\n",
    "learning agent on a different type of input (image pixels) or Gym environment. You can just choose one of\n",
    "the advanced tasks to get the maximum grade. If you are interested in improving your skills, feel free to try\n",
    "both advanced tasks. Given the higher computational complexity of RL, in this homework you don’t need to\n",
    "tune learning hyperparameters using search procedures and cross-validation; however, you are encouraged\n",
    "to play with model hyperparameters to find a satisfactory configuration.\n",
    "\n",
    "\n",
    "- 3 pt: use the notebook of Lab 07 to study how the exploration profile (either using eps-greedy or\n",
    "softmax) impacts the learning curve. Tune a bit the model hyperparameters or tweak the reward\n",
    "function to speed-up learning convergence (i.e., reach the same accuracy with fewer training episodes).\n",
    "\n",
    "YOU CAN DO JUST 1 OR BOTH\n",
    "- 5 pt: extend the notebook used in Lab 07, in order to learn to control the CartPole environment using\n",
    "directly the screen pixels, rather than the compact state representation used during the Lab (cart\n",
    "position, cart velocity, pole angle, pole angular velocity). NB: this will require to change the\n",
    "“observation_space” and to look for smart ways of encoding the pixels in a compact way to reduce\n",
    "computational complexity (e.g., crop the image around the pole, use difference of consecutive frames\n",
    "as input to consider temporal context, etc.).\n",
    "\n",
    "OR\n",
    "\n",
    "- 5 pt: train a deep RL agent on a different Gym environment. You are free to choose whatever Gym\n",
    "environment you like from the available list, or even explore other simulation platforms."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
