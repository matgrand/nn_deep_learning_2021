{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOMEWORK 2 NEURAL NETWORKS AND DEEP LEARNING\n",
    "\n",
    "---\n",
    "A.A. 2021/22 (6 CFU) - Dr. Alberto Testolin, Dr. Umberto Michieli\n",
    "---\n",
    "Student: Matteo Grandin\n",
    "---\n",
    "id: 2020374"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## v1 is for trying with a 10 elements vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General overview\n",
    " In this homework you will learn how to implement and test neural network models for\n",
    "solving unsupervised problems. For simplicity and to allow continuity with the kind of data you have seen\n",
    "before, the homework will be based on images of FashionMNIST. However, you can optionally explore\n",
    "different image collections (e.g., Caltech or Cifar) or other datasets based on your interests. The basic tasks\n",
    "for the homework will require to test and analyze the convolutional autoencoder implemented during the\n",
    "Lab practice. If you prefer, you can opt for a fully-connected autoencoder, which should achieve similar\n",
    "performance considering the relatively small size of the FashionMNIST images. As for the previous\n",
    "homework, you should explore the use of advanced optimizers and regularization methods. Learning\n",
    "hyperparameters should be tuned using appropriate search procedures, and final accuracy should be\n",
    "evaluated using a cross-validation setup. More advanced tasks will require the exploration of denoising and\n",
    "variational / adversarial architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional autoencoder\n",
    "- implement and test (convolutional) autoencoder, reporting the trend of reconstruction loss and\n",
    "some examples of image reconstruction; explore advanced optimizers and regularization methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # plotting library\n",
    "import numpy as np # this module is useful to work with numerical arrays\n",
    "import pandas as pd # this module is useful to work with tabular data\n",
    "import random # this module will be used to select random samples from a collection\n",
    "import os # this module will be used just to create directories in the local filesystem\n",
    "from tqdm import tqdm # this module is useful to plot progress bars\n",
    "from sklearn.model_selection import KFold # this module is useful to split data into training and test sets\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset\n",
    "# training and validation will be performed on the training dataset\n",
    "train_dataset = torchvision.datasets.FashionMNIST('data', train=True, download=True)\n",
    "# test dataset will only be used for evaluating final model performance\n",
    "test_dataset  = torchvision.datasets.FashionMNIST('data', train=False, download=True)\n",
    "\n",
    "label_names=['t-shirt','trouser','pullover','dress','coat','sandal','shirt',\n",
    "             'sneaker','bag','boot']\n",
    "num_labels = len(label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data transformation\n",
    "train_transform = transforms.Compose([\n",
    "    # OneHotEncoder(num_classes=10),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    # OneHotEncoder(num_classes=10),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Set the train transform\n",
    "train_dataset.transform = train_transform\n",
    "# Set the test transform\n",
    "test_dataset.transform = test_transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model definition\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, encoded_space_dim):\n",
    "        super().__init__()\n",
    "        ### Convolutional section\n",
    "        self.encoder_cnn = nn.Sequential(\n",
    "            # First convolutional layer\n",
    "            nn.Conv2d(in_channels= 1, out_channels=8, kernel_size=3, \n",
    "                      stride=2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            # Second convolutional layer\n",
    "            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, \n",
    "                      stride=2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            # Third convolutional layer\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, \n",
    "                      stride=2, padding=0),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        ### Flatten layer\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "        ### Linear section\n",
    "        self.encoder_lin = nn.Sequential(\n",
    "            # First linear layer\n",
    "            nn.Linear(in_features=32*3*3, out_features=64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            # Second linear layer\n",
    "            nn.Linear(in_features=64, out_features=32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            #third linear layer\n",
    "            nn.Linear(in_features=32, out_features=encoded_space_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # Apply convolutions\n",
    "        x = self.encoder_cnn(x)\n",
    "        # Flatten\n",
    "        x = self.flatten(x)\n",
    "        # # Apply linear layers\n",
    "        x = self.encoder_lin(x)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, encoded_space_dim):\n",
    "        super().__init__()\n",
    "        ### Linear section\n",
    "        self.decoder_lin = nn.Sequential(\n",
    "            # First linear layer\n",
    "            nn.Linear(in_features=encoded_space_dim, out_features=32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            # Second linear layer\n",
    "            nn.Linear(in_features=32, out_features=64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            # Second linear layer\n",
    "            nn.Linear(in_features=64, out_features=3*3*32),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        ### Unflatten\n",
    "        self.unflatten = nn.Unflatten(dim=1, unflattened_size=(32, 3, 3))\n",
    "        ### Convolutional section\n",
    "        self.decoder_conv = nn.Sequential(\n",
    "            # First transposed convolution\n",
    "            nn.ConvTranspose2d(in_channels=32, out_channels=16, kernel_size=3, \n",
    "                               stride=2, output_padding=0),\n",
    "            nn.ReLU(True),\n",
    "            # Second transposed convolution\n",
    "            nn.ConvTranspose2d(in_channels=16, out_channels=8, kernel_size=3, \n",
    "                               stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(True),\n",
    "            # Third transposed convolution\n",
    "            nn.ConvTranspose2d(in_channels=8, out_channels=1, kernel_size=3, \n",
    "                               stride=2, padding=1, output_padding=1)\n",
    "        ) \n",
    "    def forward(self, x):\n",
    "        # Apply linear layers\n",
    "        x = self.decoder_lin(x)\n",
    "        # Unflatten\n",
    "        x = self.unflatten(x)\n",
    "        # Apply transposed convolutions\n",
    "        x = self.decoder_conv(x)\n",
    "        # Apply a sigmoid to force the output to be between 0 and 1 (valid pixel values)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training function\n",
    "def train_epoch(encoder, decoder, device, dataloader, loss_fn, optimizer):\n",
    "    # Set train mode for both the encoder and the decoder\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    train_loss = []\n",
    "    # Iterate the dataloader (we do not need the label values, this is unsupervised learning)\n",
    "    for (image_batch, _) in dataloader: # with \"_\" we just ignore the labels (the second element of the dataloader tuple)\n",
    "        # Move tensor to the proper device\n",
    "        image_batch = image_batch.to(device)\n",
    "        # Encode data\n",
    "        encoded_data = encoder(image_batch)\n",
    "        # Decode data\n",
    "        decoded_data = decoder(encoded_data)\n",
    "        # Evaluate loss\n",
    "        loss = loss_fn(decoded_data, image_batch)\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Add loss to the list\n",
    "        train_loss.append(loss.item())\n",
    "    train_loss = np.mean(train_loss)\n",
    "    #print(f\"Batch Train loss: {train_loss}\")\n",
    "    return train_loss\n",
    "\n",
    "### Testing function\n",
    "def validate_epoch(encoder, decoder, device, dataloader, loss_fn):\n",
    "    # Set evaluation mode for encoder and decoder\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    with torch.no_grad(): # No need to track the gradients\n",
    "        # Define the lists to store the outputs for each batch\n",
    "        conc_out = []\n",
    "        conc_label = []\n",
    "        for (image_batch, _) in dataloader:\n",
    "            # Move tensor to the proper device\n",
    "            image_batch = image_batch.to(device)\n",
    "            # Encode data\n",
    "            encoded_data = encoder(image_batch)\n",
    "            # Decode data\n",
    "            decoded_data = decoder(encoded_data)\n",
    "            # Append the network output and the original image to the lists\n",
    "            conc_out.append(decoded_data.cpu())\n",
    "            conc_label.append(image_batch.cpu())\n",
    "        # Create a single tensor with all the values in the lists\n",
    "        conc_out = torch.cat(conc_out)\n",
    "        conc_label = torch.cat(conc_label) \n",
    "        # Evaluate global loss\n",
    "        val_loss = loss_fn(conc_out, conc_label)\n",
    "        #print(f\"Batch Validation loss: {val_loss}\")\n",
    "    return val_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#useful functions\n",
    "def plot_losses(train_losses, val_losses):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(train_losses, label=\"Training loss\")\n",
    "    plt.plot(val_losses, label=\"Validation loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def reset_weights(model):\n",
    "    for layer in model.modules():\n",
    "        if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
    "            layer.reset_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize hyperparameters\n",
    "- 1 pt: optimize hyperparameters using grid/random search or automatic tuning tools (e.g., Optuna)\n",
    "- final accuracy should be evaluated using a cross-validation setup (concatenate training and test set like in here, than evaluate accuracy for each fold and take the avg https://www.machinecurve.com/index.php/2021/02/03/how-to-use-k-fold-cross-validation-with-pytorch/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected device: cuda\n"
     ]
    }
   ],
   "source": [
    "#decide to load or not the pretrained model\n",
    "load_good_model = False\n",
    "\n",
    "pretrained_encoder_path = \"good_models/encoder_0_final_39.pt\"\n",
    "pretrained_decoder_path = \"good_models/decoder_0_final_39.pt\"\n",
    "\n",
    "#create a folder called training to save the model\n",
    "if not load_good_model:\n",
    "    if not os.path.exists('data/training'):\n",
    "        os.makedirs('data/training')\n",
    "    #clear the training folder\n",
    "    if os.listdir('data/training'):\n",
    "        for f in os.listdir('data/training'):\n",
    "            os.remove(os.path.join('data/training', f))\n",
    "\n",
    "# Check if the GPU is available\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f'Selected device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "#create param combinations for grid search parameters tuning\n",
    "lr = 1e-3\n",
    "encoded_space_dim = 10\n",
    "param_combinations = [[lr, 0, 0, encoded_space_dim]]#,[1e-4, 0,0, 2], [1e-3, 0, 0, 8], [1e-4, 0,0, 8]]\n",
    "print(len(param_combinations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Main block\n",
    "k_folds = 2\n",
    "num_epochs = 10\n",
    "def train_loop(param_combinations, k_folds, num_epochs, train_dataset, device = \"cuda\"):\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    encoder,decoder = Encoder(2), Decoder(2)\n",
    "\n",
    "    for comb, params in enumerate(param_combinations):\n",
    "        print(\"_____________________________________________\")\n",
    "        print(f\"Parameter combination {comb}: {params}\")\n",
    "        lr, par2, par3, encoded_space_dim = params\n",
    "\n",
    "        # initialize models\n",
    "        enc = Encoder(encoded_space_dim)\n",
    "        dec = Decoder(encoded_space_dim)\n",
    "        #reset weights\n",
    "        enc.apply(reset_weights)\n",
    "        dec.apply(reset_weights)\n",
    "        # initialize optimizer\n",
    "        params_to_optimize = [\n",
    "            {'params': enc.parameters()},\n",
    "            {'params': dec.parameters()}\n",
    "        ]\n",
    "        optim = torch.optim.Adam(params_to_optimize, lr=lr, weight_decay=1e-5)\n",
    "        # initialize loss function\n",
    "        loss_fn = nn.MSELoss()\n",
    "        # move to device\n",
    "        enc.to(device)\n",
    "        dec.to(device)\n",
    "\n",
    "        ## train the model\n",
    "        # perform cross validation \n",
    "        kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "        #initialize trainining and validation losses\n",
    "        comb_train_losses = np.zeros(num_epochs)\n",
    "        comb_val_losses = np.zeros(num_epochs)\n",
    "        for fold, (train_ids, validation_ids) in enumerate(kfold.split(train_dataset)):\n",
    "            #print(f\"Fold {fold}\")\n",
    "            #reset weights\n",
    "            enc.apply(reset_weights)\n",
    "            dec.apply(reset_weights)\n",
    "            # Sample elements randomly from a given list of ids, no replacement.\n",
    "            train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "            validation_subsampler = torch.utils.data.SubsetRandomSampler(validation_ids)\n",
    "            # dataloaders\n",
    "            train_dataloader = DataLoader(train_dataset, batch_size=256, sampler=train_subsampler) \n",
    "            validation_dataloader = DataLoader(train_dataset, batch_size=256, sampler=validation_subsampler) \n",
    "            # train the model\n",
    "            i_train_losses = []\n",
    "            i_val_losses = []\n",
    "            for epoch in tqdm(range(num_epochs)):\n",
    "                #print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "                #train\n",
    "                epoch_train_loss = train_epoch(enc, dec, device, train_dataloader, loss_fn, optim)\n",
    "                #validate\n",
    "                epoch_val_loss = validate_epoch(enc, dec, device, validation_dataloader, loss_fn)\n",
    "                # store losses\n",
    "                i_train_losses.append(epoch_train_loss)\n",
    "                i_val_losses.append(epoch_val_loss)\n",
    "                # # save model\n",
    "                # torch.save(enc.state_dict(), f\"data/training/encoder_{comb}_{fold}_{epoch}.pt\")\n",
    "                # torch.save(dec.state_dict(), f\"data/training/decoder_{comb}_{fold}_{epoch}.pt\")\n",
    "            \n",
    "            comb_train_losses += np.array(i_train_losses)/k_folds\n",
    "            comb_val_losses += np.array(i_val_losses)/k_folds\n",
    "        \n",
    "        plot_losses(comb_train_losses, comb_val_losses)\n",
    "        # train and validation loss for paramters combination\n",
    "        comb_train_loss = comb_train_losses[-1] # last epoch\n",
    "        comb_val_loss = comb_val_losses[-1] # last epoch\n",
    "        print(f\"\\n\\n\\nCombination {comb} Train loss: {comb_train_loss}\")\n",
    "        print(f\"Combination {comb} Validation loss: {comb_val_loss}\")\n",
    "\n",
    "        # train the model on all the trainign data\n",
    "        enc.apply(reset_weights)\n",
    "        dec.apply(reset_weights)\n",
    "        for epoch in tqdm(range(num_epochs)):\n",
    "            #print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "            #train\n",
    "            epoch_train_loss = train_epoch(enc, dec, device, train_dataloader, loss_fn, optim)\n",
    "            encoder = enc\n",
    "            decoder = dec\n",
    "            # save model\n",
    "            torch.save(enc.state_dict(), f\"data/training/encoder_{comb}_final_{epoch}.pt\")\n",
    "            torch.save(dec.state_dict(), f\"data/training/decoder_{comb}_final_{epoch}.pt\")\n",
    "\n",
    "\n",
    "\n",
    "    # save losses\n",
    "    with open(f\"data/training/losses_{comb}.pkl\", 'wb') as f:\n",
    "        pickle.dump([train_losses, val_losses], f)\n",
    "\n",
    "    return encoder, decoder\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select best combination #### IMPLEMENT THIS ################################\n",
    "encoded_space_dim = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________________________________________\n",
      "Parameter combination 0: [0.001, 0, 0, 10]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4797/3841811466.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_space_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_space_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mload_good_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_combinations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_folds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m#load a good model from the good_models folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4797/3246307765.py\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(param_combinations, k_folds, num_epochs, train_dataset, device)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# move to device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0menc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mdec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nn_deep_learning_2021/dl_env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    897\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m~/nn_deep_learning_2021/dl_env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nn_deep_learning_2021/dl_env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nn_deep_learning_2021/dl_env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nn_deep_learning_2021/dl_env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    895\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    896\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 897\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "# execute the training loop\n",
    "encoder, decoder = Encoder(encoded_space_dim), Decoder(encoded_space_dim)\n",
    "if not load_good_model:\n",
    "    encoder, decoder = train_loop(param_combinations, k_folds, num_epochs, train_dataset)\n",
    "else:\n",
    "    #load a good model from the good_models folder\n",
    "    encoder.load_state_dict(torch.load(pretrained_encoder_path))\n",
    "    decoder.load_state_dict(torch.load(pretrained_decoder_path))\n",
    "    print(\"Loaded pretrained model\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test the model\n",
    "# calculate loss on training set\n",
    "encoder, decoder = encoder.to(device), decoder.to(device)\n",
    "train_loss = validate_epoch(encoder, decoder, device, DataLoader(train_dataset, batch_size=60000), nn.MSELoss())\n",
    "print(f\"Train loss: {train_loss}\")\n",
    "# calculate loss on test set\n",
    "test_loss = validate_epoch(encoder, decoder, device, DataLoader(test_dataset, batch_size=20000), nn.MSELoss())\n",
    "print(f\"Test loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test on a single example\n",
    "def test_on_example(n_example, test_dataset=test_dataset, decoder=decoder, encoder=encoder):\n",
    "    # Get the output of a specific image (the test image at index 0 in this case)\n",
    "    img = test_dataset[n_example][0].unsqueeze(0).to(device)\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    with torch.no_grad():\n",
    "        rec_img  = decoder(encoder(img))\n",
    "    # Plot the reconstructed image\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(6,3))\n",
    "    axs[0].imshow(img.cpu().squeeze().numpy(), cmap='gist_gray')\n",
    "    axs[0].set_title('Original image')\n",
    "    axs[1].imshow(rec_img.cpu().squeeze().numpy(), cmap='gist_gray')\n",
    "    axs[1].set_title('Reconstructed image')\n",
    "    plt.tight_layout()\n",
    "    plt.pause(0.1)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "test_on_example(14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised fine tuning and comparison with supervised methods\n",
    "- 1 pt: fine-tune the (convolutional) autoencoder using a supervised classification task, and compare\n",
    "classification accuracy and learning speed with results achieved in Homework 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get model accuracy\n",
    "def get_accuracy(encoder, dataset):\n",
    "    this_dataloader = DataLoader(dataset, batch_size=20000)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for (data, label) in tqdm(this_dataloader):\n",
    "            # target = onehot encoding of label\n",
    "            #target = torch.eye(10)[label] \n",
    "            # Move the input and target data to the selected device\n",
    "            data, label = data.to(device), label.to(device)\n",
    "            # Compute the output\n",
    "            output = encoder(data)\n",
    "            # Get the index of the max log-probability\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            # Add to the total number of correct predictions\n",
    "            correct += pred.eq(label.view_as(pred)).sum() #.item()\n",
    "            # Add to the total number of predictions\n",
    "            total += data.shape[0]\n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "mse_multiplier = 8\n",
    "\n",
    "#load model\n",
    "if load_good_model:\n",
    "    decoder.load_state_dict(torch.load('good_models/decoder10_20.pt'))\n",
    "    encoder.load_state_dict(torch.load('good_models/encoder10_20.pt'))\n",
    "    print(\"Loaded pretrained models\")\n",
    "else:\n",
    "    # dataloader\n",
    "    train_dataloader_2 = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "    #loss\n",
    "    loss_fn_2 = nn.CrossEntropyLoss()\n",
    "    dec_loss_fn = nn.MSELoss()\n",
    "    #optimizer\n",
    "    params_to_optimize = [\n",
    "                {'params': encoder.parameters()},\n",
    "                {'params': decoder.parameters()}\n",
    "            ]\n",
    "    optimizer_2 = torch.optim.Adam(params_to_optimize, lr=1e-3, weight_decay=1e-5)\n",
    "    # to gpu\n",
    "    encoder.to(device)\n",
    "    decoder.to(device)\n",
    "\n",
    "    #train loop\n",
    "    step2_num_epochs = 20\n",
    "    target_losses = []\n",
    "    reconstruction_losses = []\n",
    "    for epoch in range(step2_num_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{step2_num_epochs}\")\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        epoch_losses2 = []\n",
    "        dec_losses = []\n",
    "        for (data, label) in tqdm(train_dataloader_2):\n",
    "            # Zero the gradients\n",
    "            optimizer_2.zero_grad()\n",
    "            ## DECODER + SINGLE LAYER MATCHER TRAINING\n",
    "            # target = onehot encoding of label\n",
    "            target = torch.eye(10)[label] \n",
    "            # Move the input and target data to the selected device\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            # use pretrained encoder to encode the data\n",
    "            encoded_data = encoder(data)\n",
    "            assert encoded_data.shape == target.shape\n",
    "            # Compute the loss\n",
    "            loss2 = loss_fn_2(encoded_data, target)\n",
    "            # Compute the gradients\n",
    "            loss2.backward(retain_graph=True)\n",
    "            ## DECODER TRAINING\n",
    "            # Compute the output\n",
    "            decoded_data = decoder(encoded_data)\n",
    "            # Compute the loss\n",
    "            dec_loss = mse_multiplier*dec_loss_fn(decoded_data, data) # for more weightage\n",
    "            # Compute the gradients\n",
    "            dec_loss.backward()\n",
    "\n",
    "            # Update the weights\n",
    "            optimizer_2.step()\n",
    "\n",
    "            #batch loss\n",
    "            loss_batch2 = loss2.item()\n",
    "            epoch_losses2.append(loss_batch2)\n",
    "            dec_loss_batch = dec_loss.item()\n",
    "            dec_losses.append(dec_loss_batch)\n",
    "\n",
    "        # Return the average training loss\n",
    "        epoch_loss2 = np.mean(epoch_losses2)\n",
    "        print(f\"target loss: {epoch_loss2}\")\n",
    "        target_losses.append(epoch_loss2)\n",
    "        dec_loss = np.mean(dec_losses)\n",
    "        print(f\"reconstruction loss (x{mse_multiplier}): {dec_loss}\")\n",
    "        reconstruction_losses.append(dec_loss)\n",
    "\n",
    "    #plot losses\n",
    "    plt.plot(target_losses)\n",
    "    plt.plot(reconstruction_losses)\n",
    "    plt.legend(['target loss', 'reconstruction loss'])\n",
    "    plt.show()\n",
    "\n",
    "#accuracy\n",
    "step_2_train_accuracy = get_accuracy(encoder, train_dataset)\n",
    "step_2_test_accuracy = get_accuracy(encoder, test_dataset)\n",
    "#print train and test accuracy with 3 decimal places\n",
    "print(f\"Train Accuracy WITH Decoder tuning: {round(step_2_train_accuracy.item()*100, 1)} %\")\n",
    "print(f\"Test Accuracy WITH Decoder tuning: {round(step_2_test_accuracy.item()*100, 1)} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "if not load_good_model:\n",
    "    torch.save(decoder.state_dict(), f'good_models/decoder10_{step2_num_epochs}.pt')\n",
    "    torch.save(encoder.state_dict(), f'good_models/encoder10_{step2_num_epochs}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test on examples\n",
    "for i in range(20,25):\n",
    "    test_on_example(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent space exploration and generation of new samples\n",
    "- 2 pt: explore the latent space structure (e.g., PCA, t-SNE) and generate new samples from latent codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational autoencoder / GAN / SimCLR\n",
    "- 2 pt: implement and test variational (convolutional) autoencoder or GAN or SimCLR\n",
    "\n",
    "Implemented in a new notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MISSING:\n",
    "# - ADVANCED OPTIMIZERS AND REGULARIZATION\n",
    "# - PARAMETERS GRID SEARCH\n",
    "# - FINAL PERFORMANCES EVALUATED IN A CROSS VALIDATION SETUP"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cee89b7c6bc96453738565335b56b694d8a30ac65e979633b683f8408c8233c6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('dl_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
